{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/c/leaf-classification/submissions",
    "#AUC ~0.26589\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                   990\n",
       "unique                   99\n",
       "top       Quercus_Palustris\n",
       "freq                     10\n",
       "Name: species, dtype: object"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['species'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('species',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   margin1   margin2   margin3   margin4   margin5   margin6   margin7  \\\n",
       "0   4  0.019531  0.009766  0.078125  0.011719  0.003906  0.015625  0.005859   \n",
       "1   7  0.007812  0.005859  0.064453  0.009766  0.003906  0.013672  0.007812   \n",
       "2   9  0.000000  0.000000  0.001953  0.021484  0.041016  0.000000  0.023438   \n",
       "3  12  0.000000  0.000000  0.009766  0.011719  0.017578  0.000000  0.003906   \n",
       "4  13  0.001953  0.000000  0.015625  0.009766  0.039062  0.000000  0.009766   \n",
       "\n",
       "   margin8   margin9  ...  texture55  texture56  texture57  texture58  \\\n",
       "0      0.0  0.005859  ...   0.006836   0.000000   0.015625   0.000977   \n",
       "1      0.0  0.033203  ...   0.000000   0.000000   0.006836   0.001953   \n",
       "2      0.0  0.011719  ...   0.128910   0.000000   0.000977   0.000000   \n",
       "3      0.0  0.003906  ...   0.012695   0.015625   0.002930   0.036133   \n",
       "4      0.0  0.005859  ...   0.000000   0.042969   0.016602   0.010742   \n",
       "\n",
       "   texture59  texture60  texture61  texture62  texture63  texture64  \n",
       "0   0.015625        0.0        0.0   0.000000   0.003906   0.053711  \n",
       "1   0.013672        0.0        0.0   0.000977   0.037109   0.044922  \n",
       "2   0.000000        0.0        0.0   0.015625   0.000000   0.000000  \n",
       "3   0.013672        0.0        0.0   0.089844   0.000000   0.008789  \n",
       "4   0.041016        0.0        0.0   0.007812   0.009766   0.007812  \n",
       "\n",
       "[5 rows x 193 columns]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 990 entries, 0 to 989\n",
      "Columns: 192 entries, margin1 to texture64\n",
      "dtypes: float64(192)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import np_utils\n",
    "# Y_train = np_utils.to_categorical(y, 10)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE = LabelEncoder()\n",
    "y = LE.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 49, 65, 94, 84, 40, 54, 78, 53, 89, 98, 16, 74, 50, 58, 31, 43,\n",
       "        4, 75, 44, 83, 84, 13, 66, 15,  6, 73, 22, 73, 31, 36, 27, 94, 88,\n",
       "       12, 28, 21, 25, 20, 60, 84, 65, 69, 58, 23, 76, 18, 52, 54,  9, 48,\n",
       "       47, 64, 81, 83, 36, 58, 21, 81, 20, 62, 88, 34, 92, 79, 82, 20, 32,\n",
       "        4, 84, 36, 35, 72, 60, 71, 72, 52, 50, 54, 11, 51, 18, 47,  5,  8,\n",
       "       37, 97, 20, 33,  1, 59,  1, 56,  1,  9, 57, 20, 79, 29, 16, 32, 54,\n",
       "       93, 10, 46, 59, 84, 76, 15, 10, 15,  0, 69,  4, 51, 51, 94, 36, 39,\n",
       "       62,  2, 24, 26, 35, 25, 87,  0, 55, 34, 38,  1, 45,  7, 93, 56, 38,\n",
       "       21, 51, 75, 81, 74, 33, 20, 37,  9, 40, 60, 31, 83, 50, 71, 67, 30,\n",
       "       66,  1, 43, 61, 23, 65, 84, 87, 46, 57, 16,  2, 28, 12, 96, 44, 76,\n",
       "       29, 75, 41, 87, 67, 61, 30,  5, 12, 62,  3, 83, 81,  6, 85,  4, 37,\n",
       "       57, 84, 39, 71, 61,  6, 76, 14, 31, 98, 40, 17, 51, 16, 42, 63, 86,\n",
       "       37, 69, 86, 71, 80, 78, 14, 35, 25,  5, 39,  8,  9, 26, 44, 60, 13,\n",
       "       14, 77, 13, 80, 87, 18, 60, 78, 92, 51, 45, 78, 41, 51, 30, 14, 35,\n",
       "       46, 21,  8,  6, 92, 38, 40, 15, 32, 17, 93, 71, 92, 27, 78, 15, 19,\n",
       "       60, 21, 38, 36, 49, 74, 67, 95, 31, 82, 45, 16, 83, 63, 80, 42, 22,\n",
       "       74, 53, 15, 44, 47, 57, 94, 76, 17, 32, 24, 15, 93, 24, 80, 59, 46,\n",
       "       12, 51, 77, 79, 70, 69, 16,  2, 63, 83, 55, 12, 53,  1, 67,  0,  2,\n",
       "       36, 42, 10,  9, 52, 59,  6, 22, 86, 31, 51, 37, 43, 75, 90, 24, 86,\n",
       "       96, 45, 32, 98, 36, 66, 48, 73, 73, 79, 56, 41, 21, 25, 27, 97, 18,\n",
       "       44, 45, 40, 80, 63, 20, 35,  0,  8, 27, 25, 35, 59, 61, 21, 37, 29,\n",
       "        6, 19, 78, 50, 54, 37, 93, 33, 46, 79, 59, 29, 43,  0, 23, 17, 38,\n",
       "       66, 38, 89, 17, 25, 31, 65, 10, 26, 86, 58, 42, 46, 24, 95, 93,  8,\n",
       "       53, 32, 14, 10, 94,  8,  8, 64, 44, 74, 30, 97, 22, 11, 68, 56, 90,\n",
       "       96, 16, 43, 57, 91, 24, 28, 82, 90, 64, 61, 92, 28, 84, 70, 45, 85,\n",
       "       34,  7, 88, 89, 61, 26, 88, 41, 46,  8, 91, 41, 14, 98, 28, 26, 36,\n",
       "       70, 74,  7, 52, 70, 42, 66, 22, 13, 44, 91, 53, 22, 16, 40, 40, 28,\n",
       "       70,  6, 60, 95, 23, 16, 50, 29, 49,  9, 18, 55, 63, 60, 19, 28, 30,\n",
       "       31, 85, 66, 88, 63, 83, 64, 96, 13, 34, 27, 95, 36, 72, 29, 91, 22,\n",
       "       65, 71, 66, 11, 32,  2, 75, 39,  5, 37, 67, 81, 55, 61, 57, 81, 82,\n",
       "       63, 55, 54, 35, 86, 25, 24, 96, 10, 58, 59, 28, 89, 54, 52, 85, 68,\n",
       "       69,  8, 39, 95, 39, 82, 48, 74, 52, 74, 55,  9, 47, 84, 91, 12, 96,\n",
       "       82, 64,  7, 40, 73, 77, 11, 36, 68, 23, 28, 46, 75, 43,  2, 11, 47,\n",
       "       53, 56, 62, 62, 80, 56, 30,  3, 88, 37, 33, 73, 76, 21,  5, 76, 87,\n",
       "       68, 83, 62, 57, 47, 19, 88, 96, 42, 23, 44, 87, 82, 49, 63, 24, 94,\n",
       "       69, 54,  5, 79, 43, 12, 50,  5, 52, 92,  4, 84,  1, 33, 49, 26, 18,\n",
       "       44, 13, 24, 73, 89, 78, 67, 41, 11, 46, 47, 69,  0, 18, 98, 44, 85,\n",
       "       29, 53,  1, 45,  3,  9, 13,  2, 66, 59, 79,  6, 17, 43, 83, 26,  1,\n",
       "       12, 49, 71, 89, 58, 93, 39, 42, 15, 38, 55, 15, 93,  4, 90, 88, 55,\n",
       "       40, 55, 17, 34, 94, 57, 92, 81, 26, 60, 89, 49, 89, 30, 65, 58,  4,\n",
       "       19,  4, 76, 74, 71, 21, 54, 13, 16, 72, 68, 62, 61, 25, 72,  7, 12,\n",
       "       18, 77, 90, 62, 14,  3, 78, 65, 37, 27, 50, 95, 98, 60, 72, 58, 38,\n",
       "       87, 93, 19,  7, 83, 50,  3, 91, 77,  7, 64, 61, 69, 23, 76, 65, 48,\n",
       "       41, 92, 20, 91, 18, 70,  9,  9, 29, 85, 67,  0, 35, 98, 91, 90, 31,\n",
       "       53, 39, 24, 85, 96, 17,  7, 11, 96, 39, 56, 90, 79, 45, 64, 97, 41,\n",
       "       19, 74, 11, 10, 62, 95, 28, 96, 10,  7, 68,  7, 93, 34, 42, 68, 41,\n",
       "       14, 22, 58, 12, 71, 27, 98, 72, 91,  3, 43, 19, 61, 75, 20, 81, 63,\n",
       "       67, 56, 26, 47, 11, 31, 57, 62, 66, 19, 75, 97, 94, 13, 75, 95, 32,\n",
       "       50, 97, 52, 87, 32,  3, 47, 77, 48, 33, 73, 64, 49, 68, 43, 94, 77,\n",
       "       68, 47, 82,  2, 30, 23, 33, 34, 66, 33, 35, 88, 68, 27, 87, 54, 79,\n",
       "       34, 67, 65, 18,  4, 26, 30, 52, 86,  0, 29, 80, 67, 95, 39, 25, 70,\n",
       "       58, 35, 27, 17, 38, 91, 13, 23, 77, 79, 77, 22, 49, 98, 48, 46, 48,\n",
       "        5, 63, 97, 80, 53, 20, 25, 78, 10, 65, 33, 41, 85, 90, 98, 97, 71,\n",
       "       95, 52,  3, 29, 69, 51, 70, 27, 22, 34,  6, 48, 72, 21, 89, 17, 97,\n",
       "       72, 80, 10, 57, 64, 92, 38, 15, 73, 87, 73, 48, 42, 82, 33, 56,  3,\n",
       "       42,  1, 53, 55, 90, 19,  6, 30, 86, 64, 49,  2,  8, 45, 76, 92,  0,\n",
       "       23, 69, 59, 80, 90, 32,  5, 59, 85, 89, 94, 45, 48, 86, 81, 14,  4,\n",
       "       77, 56, 82,  2, 85, 70, 88,  0, 75, 14, 86, 81, 97, 70, 72, 34, 40,\n",
       "        5, 11, 78, 50])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(100,input_dim = 192,activation = 'tanh'))\n",
    "model.add(keras.layers.Dense(100,activation = 'relu'))\n",
    "model.add(keras.layers.Dense(99,activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/54\n",
      "990/990 [==============================] - 3s 3ms/sample - loss: 4.5987 - acc: 0.0061\n",
      "Epoch 2/54\n",
      "990/990 [==============================] - 1s 787us/sample - loss: 4.5965 - acc: 0.0071\n",
      "Epoch 3/54\n",
      "990/990 [==============================] - 1s 741us/sample - loss: 4.5487 - acc: 0.0141\n",
      "Epoch 4/54\n",
      "990/990 [==============================] - 1s 759us/sample - loss: 4.1495 - acc: 0.0152\n",
      "Epoch 5/54\n",
      "990/990 [==============================] - 1s 749us/sample - loss: 3.9504 - acc: 0.0182\n",
      "Epoch 6/54\n",
      "990/990 [==============================] - 1s 747us/sample - loss: 3.7405 - acc: 0.0192\n",
      "Epoch 7/54\n",
      "990/990 [==============================] - 1s 784us/sample - loss: 3.5897 - acc: 0.0303\n",
      "Epoch 8/54\n",
      "990/990 [==============================] - 1s 761us/sample - loss: 3.4912 - acc: 0.0303\n",
      "Epoch 9/54\n",
      "990/990 [==============================] - 1s 754us/sample - loss: 3.4350 - acc: 0.0343\n",
      "Epoch 10/54\n",
      "990/990 [==============================] - 1s 760us/sample - loss: 3.3621 - acc: 0.0414\n",
      "Epoch 11/54\n",
      "990/990 [==============================] - 1s 917us/sample - loss: 3.3094 - acc: 0.0495\n",
      "Epoch 12/54\n",
      "990/990 [==============================] - 1s 837us/sample - loss: 3.2800 - acc: 0.0505\n",
      "Epoch 13/54\n",
      "990/990 [==============================] - 1s 767us/sample - loss: 3.2560 - acc: 0.0404\n",
      "Epoch 14/54\n",
      "990/990 [==============================] - 1s 846us/sample - loss: 3.2165 - acc: 0.0404\n",
      "Epoch 15/54\n",
      "990/990 [==============================] - 1s 805us/sample - loss: 3.1573 - acc: 0.0556\n",
      "Epoch 16/54\n",
      "990/990 [==============================] - 1s 775us/sample - loss: 3.1539 - acc: 0.0444\n",
      "Epoch 17/54\n",
      "990/990 [==============================] - 1s 770us/sample - loss: 3.1276 - acc: 0.0475\n",
      "Epoch 18/54\n",
      "990/990 [==============================] - 1s 861us/sample - loss: 3.0958 - acc: 0.0606\n",
      "Epoch 19/54\n",
      "990/990 [==============================] - 1s 903us/sample - loss: 3.0777 - acc: 0.0646\n",
      "Epoch 20/54\n",
      "990/990 [==============================] - 1s 826us/sample - loss: 3.0601 - acc: 0.0596\n",
      "Epoch 21/54\n",
      "990/990 [==============================] - 1s 807us/sample - loss: 3.0518 - acc: 0.0545\n",
      "Epoch 22/54\n",
      "990/990 [==============================] - 1s 779us/sample - loss: 3.0188 - acc: 0.0646\n",
      "Epoch 23/54\n",
      "990/990 [==============================] - 1s 785us/sample - loss: 2.9977 - acc: 0.0747\n",
      "Epoch 24/54\n",
      "990/990 [==============================] - 1s 799us/sample - loss: 2.9878 - acc: 0.0525\n",
      "Epoch 25/54\n",
      "990/990 [==============================] - 1s 766us/sample - loss: 2.9687 - acc: 0.0626\n",
      "Epoch 26/54\n",
      "990/990 [==============================] - 1s 795us/sample - loss: 2.9911 - acc: 0.0717\n",
      "Epoch 27/54\n",
      "990/990 [==============================] - 1s 788us/sample - loss: 2.9408 - acc: 0.0818\n",
      "Epoch 28/54\n",
      "990/990 [==============================] - 1s 764us/sample - loss: 2.9382 - acc: 0.0899\n",
      "Epoch 29/54\n",
      "990/990 [==============================] - 1s 788us/sample - loss: 2.9103 - acc: 0.0758\n",
      "Epoch 30/54\n",
      "990/990 [==============================] - 1s 790us/sample - loss: 2.9364 - acc: 0.0758\n",
      "Epoch 31/54\n",
      "990/990 [==============================] - 1s 774us/sample - loss: 2.8849 - acc: 0.0747\n",
      "Epoch 32/54\n",
      "990/990 [==============================] - 1s 795us/sample - loss: 2.8732 - acc: 0.0828\n",
      "Epoch 33/54\n",
      "990/990 [==============================] - 1s 783us/sample - loss: 2.8727 - acc: 0.0586\n",
      "Epoch 34/54\n",
      "990/990 [==============================] - 1s 782us/sample - loss: 2.8528 - acc: 0.0677\n",
      "Epoch 35/54\n",
      "990/990 [==============================] - 1s 789us/sample - loss: 2.8168 - acc: 0.0848\n",
      "Epoch 36/54\n",
      "990/990 [==============================] - 1s 821us/sample - loss: 2.8192 - acc: 0.1020\n",
      "Epoch 37/54\n",
      "990/990 [==============================] - 1s 813us/sample - loss: 2.7885 - acc: 0.0879\n",
      "Epoch 38/54\n",
      "990/990 [==============================] - 1s 791us/sample - loss: 2.8188 - acc: 0.1000\n",
      "Epoch 39/54\n",
      "990/990 [==============================] - 1s 791us/sample - loss: 2.7693 - acc: 0.1051\n",
      "Epoch 40/54\n",
      "990/990 [==============================] - 1s 804us/sample - loss: 2.7530 - acc: 0.1061\n",
      "Epoch 41/54\n",
      "990/990 [==============================] - 1s 786us/sample - loss: 2.7198 - acc: 0.1051\n",
      "Epoch 42/54\n",
      "990/990 [==============================] - 1s 790us/sample - loss: 2.6860 - acc: 0.1162\n",
      "Epoch 43/54\n",
      "990/990 [==============================] - 1s 813us/sample - loss: 2.6949 - acc: 0.1222\n",
      "Epoch 44/54\n",
      "990/990 [==============================] - 1s 792us/sample - loss: 2.6587 - acc: 0.1111\n",
      "Epoch 45/54\n",
      "990/990 [==============================] - 1s 799us/sample - loss: 2.6073 - acc: 0.1374\n",
      "Epoch 46/54\n",
      "990/990 [==============================] - 1s 807us/sample - loss: 2.5621 - acc: 0.1364\n",
      "Epoch 47/54\n",
      "990/990 [==============================] - 1s 822us/sample - loss: 2.5056 - acc: 0.1545\n",
      "Epoch 48/54\n",
      "990/990 [==============================] - 1s 916us/sample - loss: 2.4916 - acc: 0.1616\n",
      "Epoch 49/54\n",
      "990/990 [==============================] - 1s 936us/sample - loss: 2.4279 - acc: 0.1727\n",
      "Epoch 50/54\n",
      "990/990 [==============================] - 1s 874us/sample - loss: 2.3751 - acc: 0.1909\n",
      "Epoch 51/54\n",
      "990/990 [==============================] - 1s 893us/sample - loss: 2.3370 - acc: 0.1929\n",
      "Epoch 52/54\n",
      "990/990 [==============================] - 1s 855us/sample - loss: 2.2575 - acc: 0.2374\n",
      "Epoch 53/54\n",
      "990/990 [==============================] - 1s 870us/sample - loss: 2.2949 - acc: 0.2152\n",
      "Epoch 54/54\n",
      "990/990 [==============================] - 1s 837us/sample - loss: 2.1853 - acc: 0.2475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4cdf7860>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,y,epochs=54,verbose=1,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test['id']\n",
    "predictions = model.predict(test.drop('id', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LABELS = sorted(pd.read_csv(os.path.join('train.csv')).species.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = pd.DataFrame(predictions,index=ids,columns=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating and writing submission...')\n",
    "fp = open('submit.csv', 'w')\n",
    "fp.write(yp.to_csv())\n",
    "print('Finished writing submission')\n",
    "# Display the submission\n",
    "yp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
